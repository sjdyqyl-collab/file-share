// MoE Transformer Baseline (TP=8, PP=2)
digraph {
	rankdir=TB size="20,30"
	node [color=lightblue shape=ellipse style=filled]
	node [color=lightgreen shape=rectangle style=filled]
	node [color=lightyellow shape=parallelogram style=filled]
	node [color=lightcoral shape=diamond style=filled]
	subgraph cluster_input {
		label="Input Layer"
		input [label="Input
X: [B, L, d_model]
GPU: Host" shape=parallelogram]
	}
	subgraph cluster_mha0 {
		label="Multi-Head Attention"
		qkv0_0 [label="QKV Projection 0
[Q,K,V]: [B, L/P, d_model/8]
GPU: 0" shape=rectangle]
		qkv0_1 [label="QKV Projection 1
[Q,K,V]: [B, L/P, d_model/8]
GPU: 1" shape=rectangle]
		qkv0_2 [label="QKV Projection 2
[Q,K,V]: [B, L/P, d_model/8]
GPU: 2" shape=rectangle]
		qkv0_3 [label="QKV Projection 3
[Q,K,V]: [B, L/P, d_model/8]
GPU: 3" shape=rectangle]
		qkv0_4 [label="QKV Projection 4
[Q,K,V]: [B, L/P, d_model/8]
GPU: 4" shape=rectangle]
		qkv0_5 [label="QKV Projection 5
[Q,K,V]: [B, L/P, d_model/8]
GPU: 5" shape=rectangle]
		qkv0_6 [label="QKV Projection 6
[Q,K,V]: [B, L/P, d_model/8]
GPU: 6" shape=rectangle]
		qkv0_7 [label="QKV Projection 7
[Q,K,V]: [B, L/P, d_model/8]
GPU: 7" shape=rectangle]
		attn0_0 [label="Attention 0
Output: [B, L/P, d_model/8]
GPU: 0" shape=rectangle]
		attn0_1 [label="Attention 1
Output: [B, L/P, d_model/8]
GPU: 1" shape=rectangle]
		attn0_2 [label="Attention 2
Output: [B, L/P, d_model/8]
GPU: 2" shape=rectangle]
		attn0_3 [label="Attention 3
Output: [B, L/P, d_model/8]
GPU: 3" shape=rectangle]
		attn0_4 [label="Attention 4
Output: [B, L/P, d_model/8]
GPU: 4" shape=rectangle]
		attn0_5 [label="Attention 5
Output: [B, L/P, d_model/8]
GPU: 5" shape=rectangle]
		attn0_6 [label="Attention 6
Output: [B, L/P, d_model/8]
GPU: 6" shape=rectangle]
		attn0_7 [label="Attention 7
Output: [B, L/P, d_model/8]
GPU: 7" shape=rectangle]
		out0_0 [label="Output Proj 0
Output: [B, L/P, d_model/8]
GPU: 0" shape=rectangle]
		out0_1 [label="Output Proj 1
Output: [B, L/P, d_model/8]
GPU: 1" shape=rectangle]
		out0_2 [label="Output Proj 2
Output: [B, L/P, d_model/8]
GPU: 2" shape=rectangle]
		out0_3 [label="Output Proj 3
Output: [B, L/P, d_model/8]
GPU: 3" shape=rectangle]
		out0_4 [label="Output Proj 4
Output: [B, L/P, d_model/8]
GPU: 4" shape=rectangle]
		out0_5 [label="Output Proj 5
Output: [B, L/P, d_model/8]
GPU: 5" shape=rectangle]
		out0_6 [label="Output Proj 6
Output: [B, L/P, d_model/8]
GPU: 6" shape=rectangle]
		out0_7 [label="Output Proj 7
Output: [B, L/P, d_model/8]
GPU: 7" shape=rectangle]
		ar0 [label="All-Reduce
Sum across 8 GPUs
GPU: 0-7" shape=ellipse]
	}
	subgraph cluster_expert0_0 {
		label="Expert 0"
		expert0_0_1 [label="Expert 0 Linear1
Output: [B, L/P, ffn_dim]
GPU: 0" shape=rectangle]
		expert0_0_act [label="Expert 0 GELU
GPU: 0" shape=rectangle]
		expert0_0_2 [label="Expert 0 Linear2
Output: [B, L/P, d_model]
GPU: 0" shape=rectangle]
	}
	subgraph cluster_expert0_1 {
		label="Expert 1"
		expert0_1_1 [label="Expert 1 Linear1
Output: [B, L/P, ffn_dim]
GPU: 1" shape=rectangle]
		expert0_1_act [label="Expert 1 GELU
GPU: 1" shape=rectangle]
		expert0_1_2 [label="Expert 1 Linear2
Output: [B, L/P, d_model]
GPU: 1" shape=rectangle]
	}
	subgraph cluster_expert0_2 {
		label="Expert 2"
		expert0_2_1 [label="Expert 2 Linear1
Output: [B, L/P, ffn_dim]
GPU: 2" shape=rectangle]
		expert0_2_act [label="Expert 2 GELU
GPU: 2" shape=rectangle]
		expert0_2_2 [label="Expert 2 Linear2
Output: [B, L/P, d_model]
GPU: 2" shape=rectangle]
	}
	subgraph cluster_expert0_3 {
		label="Expert 3"
		expert0_3_1 [label="Expert 3 Linear1
Output: [B, L/P, ffn_dim]
GPU: 3" shape=rectangle]
		expert0_3_act [label="Expert 3 GELU
GPU: 3" shape=rectangle]
		expert0_3_2 [label="Expert 3 Linear2
Output: [B, L/P, d_model]
GPU: 3" shape=rectangle]
	}
	subgraph cluster_expert0_4 {
		label="Expert 4"
		expert0_4_1 [label="Expert 4 Linear1
Output: [B, L/P, ffn_dim]
GPU: 4" shape=rectangle]
		expert0_4_act [label="Expert 4 GELU
GPU: 4" shape=rectangle]
		expert0_4_2 [label="Expert 4 Linear2
Output: [B, L/P, d_model]
GPU: 4" shape=rectangle]
	}
	subgraph cluster_expert0_5 {
		label="Expert 5"
		expert0_5_1 [label="Expert 5 Linear1
Output: [B, L/P, ffn_dim]
GPU: 5" shape=rectangle]
		expert0_5_act [label="Expert 5 GELU
GPU: 5" shape=rectangle]
		expert0_5_2 [label="Expert 5 Linear2
Output: [B, L/P, d_model]
GPU: 5" shape=rectangle]
	}
	subgraph cluster_expert0_6 {
		label="Expert 6"
		expert0_6_1 [label="Expert 6 Linear1
Output: [B, L/P, ffn_dim]
GPU: 6" shape=rectangle]
		expert0_6_act [label="Expert 6 GELU
GPU: 6" shape=rectangle]
		expert0_6_2 [label="Expert 6 Linear2
Output: [B, L/P, d_model]
GPU: 6" shape=rectangle]
	}
	subgraph cluster_expert0_7 {
		label="Expert 7"
		expert0_7_1 [label="Expert 7 Linear1
Output: [B, L/P, ffn_dim]
GPU: 7" shape=rectangle]
		expert0_7_act [label="Expert 7 GELU
GPU: 7" shape=rectangle]
		expert0_7_2 [label="Expert 7 Linear2
Output: [B, L/P, d_model]
GPU: 7" shape=rectangle]
	}
	subgraph cluster_moe0 {
		label="Mixture of Experts"
		gate0 [label="Gate
Compute routing scores
Input: [B, L/P, d_model]
GPU: 0-7" shape=diamond]
		select0 [label="Select Top-2 Experts
GPU: 0-7" shape=parallelogram]
		combine0 [label="Combine Expert Outputs
Weighted sum by gate scores
GPU: 0-7" shape=parallelogram]
	}
	subgraph cluster_layer0 {
		label="Layer 0"
		split0 [label="Split for Pipeline
X: [B, L/P, d_model]
GPU: 0-7" shape=parallelogram]
		res0 [label="Residual Add
Input: [B, L/P, d_model]
GPU: 0-7" shape=parallelogram]
		res_moe0 [label="Residual Add
Input: [B, L/P, d_model]
GPU: 0-7" shape=parallelogram]
	}
	subgraph cluster_stage0 {
		label="Pipeline Stage 0 (GPUs 0-7)"
	}
	send_stage0 [label="Send to Stage 1
X: [B, L/P, d_model]
GPU: 0-7 â†’ 8-15" shape=ellipse]
	subgraph cluster_mha2 {
		label="Multi-Head Attention"
		qkv2_8 [label="QKV Projection 8
[Q,K,V]: [B, L/P, d_model/8]
GPU: 8" shape=rectangle]
		qkv2_9 [label="QKV Projection 9
[Q,K,V]: [B, L/P, d_model/8]
GPU: 9" shape=rectangle]
		qkv2_10 [label="QKV Projection 10
[Q,K,V]: [B, L/P, d_model/8]
GPU: 10" shape=rectangle]
		qkv2_11 [label="QKV Projection 11
[Q,K,V]: [B, L/P, d_model/8]
GPU: 11" shape=rectangle]
		qkv2_12 [label="QKV Projection 12
[Q,K,V]: [B, L/P, d_model/8]
GPU: 12" shape=rectangle]
		qkv2_13 [label="QKV Projection 13
[Q,K,V]: [B, L/P, d_model/8]
GPU: 13" shape=rectangle]
		qkv2_14 [label="QKV Projection 14
[Q,K,V]: [B, L/P, d_model/8]
GPU: 14" shape=rectangle]
		qkv2_15 [label="QKV Projection 15
[Q,K,V]: [B, L/P, d_model/8]
GPU: 15" shape=rectangle]
		attn2_8 [label="Attention 8
Output: [B, L/P, d_model/8]
GPU: 8" shape=rectangle]
		attn2_9 [label="Attention 9
Output: [B, L/P, d_model/8]
GPU: 9" shape=rectangle]
		attn2_10 [label="Attention 10
Output: [B, L/P, d_model/8]
GPU: 10" shape=rectangle]
		attn2_11 [label="Attention 11
Output: [B, L/P, d_model/8]
GPU: 11" shape=rectangle]
		attn2_12 [label="Attention 12
Output: [B, L/P, d_model/8]
GPU: 12" shape=rectangle]
		attn2_13 [label="Attention 13
Output: [B, L/P, d_model/8]
GPU: 13" shape=rectangle]
		attn2_14 [label="Attention 14
Output: [B, L/P, d_model/8]
GPU: 14" shape=rectangle]
		attn2_15 [label="Attention 15
Output: [B, L/P, d_model/8]
GPU: 15" shape=rectangle]
		out2_8 [label="Output Proj 8
Output: [B, L/P, d_model/8]
GPU: 8" shape=rectangle]
		out2_9 [label="Output Proj 9
Output: [B, L/P, d_model/8]
GPU: 9" shape=rectangle]
		out2_10 [label="Output Proj 10
Output: [B, L/P, d_model/8]
GPU: 10" shape=rectangle]
		out2_11 [label="Output Proj 11
Output: [B, L/P, d_model/8]
GPU: 11" shape=rectangle]
		out2_12 [label="Output Proj 12
Output: [B, L/P, d_model/8]
GPU: 12" shape=rectangle]
		out2_13 [label="Output Proj 13
Output: [B, L/P, d_model/8]
GPU: 13" shape=rectangle]
		out2_14 [label="Output Proj 14
Output: [B, L/P, d_model/8]
GPU: 14" shape=rectangle]
		out2_15 [label="Output Proj 15
Output: [B, L/P, d_model/8]
GPU: 15" shape=rectangle]
		ar2 [label="All-Reduce
Sum across 8 GPUs
GPU: 8-15" shape=ellipse]
	}
	subgraph cluster_expert2_0 {
		label="Expert 8"
		expert2_0_1 [label="Expert 8 Linear1
Output: [B, L/P, ffn_dim]
GPU: 8" shape=rectangle]
		expert2_0_act [label="Expert 8 GELU
GPU: 8" shape=rectangle]
		expert2_0_2 [label="Expert 8 Linear2
Output: [B, L/P, d_model]
GPU: 8" shape=rectangle]
	}
	subgraph cluster_expert2_1 {
		label="Expert 9"
		expert2_1_1 [label="Expert 9 Linear1
Output: [B, L/P, ffn_dim]
GPU: 9" shape=rectangle]
		expert2_1_act [label="Expert 9 GELU
GPU: 9" shape=rectangle]
		expert2_1_2 [label="Expert 9 Linear2
Output: [B, L/P, d_model]
GPU: 9" shape=rectangle]
	}
	subgraph cluster_expert2_2 {
		label="Expert 10"
		expert2_2_1 [label="Expert 10 Linear1
Output: [B, L/P, ffn_dim]
GPU: 10" shape=rectangle]
		expert2_2_act [label="Expert 10 GELU
GPU: 10" shape=rectangle]
		expert2_2_2 [label="Expert 10 Linear2
Output: [B, L/P, d_model]
GPU: 10" shape=rectangle]
	}
	subgraph cluster_expert2_3 {
		label="Expert 11"
		expert2_3_1 [label="Expert 11 Linear1
Output: [B, L/P, ffn_dim]
GPU: 11" shape=rectangle]
		expert2_3_act [label="Expert 11 GELU
GPU: 11" shape=rectangle]
		expert2_3_2 [label="Expert 11 Linear2
Output: [B, L/P, d_model]
GPU: 11" shape=rectangle]
	}
	subgraph cluster_expert2_4 {
		label="Expert 12"
		expert2_4_1 [label="Expert 12 Linear1
Output: [B, L/P, ffn_dim]
GPU: 12" shape=rectangle]
		expert2_4_act [label="Expert 12 GELU
GPU: 12" shape=rectangle]
		expert2_4_2 [label="Expert 12 Linear2
Output: [B, L/P, d_model]
GPU: 12" shape=rectangle]
	}
	subgraph cluster_expert2_5 {
		label="Expert 13"
		expert2_5_1 [label="Expert 13 Linear1
Output: [B, L/P, ffn_dim]
GPU: 13" shape=rectangle]
		expert2_5_act [label="Expert 13 GELU
GPU: 13" shape=rectangle]
		expert2_5_2 [label="Expert 13 Linear2
Output: [B, L/P, d_model]
GPU: 13" shape=rectangle]
	}
	subgraph cluster_expert2_6 {
		label="Expert 14"
		expert2_6_1 [label="Expert 14 Linear1
Output: [B, L/P, ffn_dim]
GPU: 14" shape=rectangle]
		expert2_6_act [label="Expert 14 GELU
GPU: 14" shape=rectangle]
		expert2_6_2 [label="Expert 14 Linear2
Output: [B, L/P, d_model]
GPU: 14" shape=rectangle]
	}
	subgraph cluster_expert2_7 {
		label="Expert 15"
		expert2_7_1 [label="Expert 15 Linear1
Output: [B, L/P, ffn_dim]
GPU: 15" shape=rectangle]
		expert2_7_act [label="Expert 15 GELU
GPU: 15" shape=rectangle]
		expert2_7_2 [label="Expert 15 Linear2
Output: [B, L/P, d_model]
GPU: 15" shape=rectangle]
	}
	subgraph cluster_moe2 {
		label="Mixture of Experts"
		gate2 [label="Gate
Compute routing scores
Input: [B, L/P, d_model]
GPU: 8-15" shape=diamond]
		select2 [label="Select Top-2 Experts
GPU: 8-15" shape=parallelogram]
		combine2 [label="Combine Expert Outputs
Weighted sum by gate scores
GPU: 8-15" shape=parallelogram]
	}
	subgraph cluster_layer2 {
		label="Layer 2"
		recv_stage1 [label="Receive from Stage 0
X: [B, L/P, d_model]
GPU: 8-15" shape=ellipse]
		res2 [label="Residual Add
Input: [B, L/P, d_model]
GPU: 8-15" shape=parallelogram]
		res_moe2 [label="Residual Add
Input: [B, L/P, d_model]
GPU: 8-15" shape=parallelogram]
	}
	subgraph cluster_mha3 {
		label="Multi-Head Attention"
		qkv3_8 [label="QKV Projection 8
[Q,K,V]: [B, L/P, d_model/8]
GPU: 8" shape=rectangle]
		qkv3_9 [label="QKV Projection 9
[Q,K,V]: [B, L/P, d_model/8]
GPU: 9" shape=rectangle]
		qkv3_10 [label="QKV Projection 10
[Q,K,V]: [B, L/P, d_model/8]
GPU: 10" shape=rectangle]
		qkv3_11 [label="QKV Projection 11
[Q,K,V]: [B, L/P, d_model/8]
GPU: 11" shape=rectangle]
		qkv3_12 [label="QKV Projection 12
[Q,K,V]: [B, L/P, d_model/8]
GPU: 12" shape=rectangle]
		qkv3_13 [label="QKV Projection 13
[Q,K,V]: [B, L/P, d_model/8]
GPU: 13" shape=rectangle]
		qkv3_14 [label="QKV Projection 14
[Q,K,V]: [B, L/P, d_model/8]
GPU: 14" shape=rectangle]
		qkv3_15 [label="QKV Projection 15
[Q,K,V]: [B, L/P, d_model/8]
GPU: 15" shape=rectangle]
		attn3_8 [label="Attention 8
Output: [B, L/P, d_model/8]
GPU: 8" shape=rectangle]
		attn3_9 [label="Attention 9
Output: [B, L/P, d_model/8]
GPU: 9" shape=rectangle]
		attn3_10 [label="Attention 10
Output: [B, L/P, d_model/8]
GPU: 10" shape=rectangle]
		attn3_11 [label="Attention 11
Output: [B, L/P, d_model/8]
GPU: 11" shape=rectangle]
		attn3_12 [label="Attention 12
Output: [B, L/P, d_model/8]
GPU: 12" shape=rectangle]
		attn3_13 [label="Attention 13
Output: [B, L/P, d_model/8]
GPU: 13" shape=rectangle]
		attn3_14 [label="Attention 14
Output: [B, L/P, d_model/8]
GPU: 14" shape=rectangle]
		attn3_15 [label="Attention 15
Output: [B, L/P, d_model/8]
GPU: 15" shape=rectangle]
		out3_8 [label="Output Proj 8
Output: [B, L/P, d_model/8]
GPU: 8" shape=rectangle]
		out3_9 [label="Output Proj 9
Output: [B, L/P, d_model/8]
GPU: 9" shape=rectangle]
		out3_10 [label="Output Proj 10
Output: [B, L/P, d_model/8]
GPU: 10" shape=rectangle]
		out3_11 [label="Output Proj 11
Output: [B, L/P, d_model/8]
GPU: 11" shape=rectangle]
		out3_12 [label="Output Proj 12
Output: [B, L/P, d_model/8]
GPU: 12" shape=rectangle]
		out3_13 [label="Output Proj 13
Output: [B, L/P, d_model/8]
GPU: 13" shape=rectangle]
		out3_14 [label="Output Proj 14
Output: [B, L/P, d_model/8]
GPU: 14" shape=rectangle]
		out3_15 [label="Output Proj 15
Output: [B, L/P, d_model/8]
GPU: 15" shape=rectangle]
		ar3 [label="All-Reduce
Sum across 8 GPUs
GPU: 8-15" shape=ellipse]
	}
	subgraph cluster_expert3_0 {
		label="Expert 8"
		expert3_0_1 [label="Expert 8 Linear1
Output: [B, L/P, ffn_dim]
GPU: 8" shape=rectangle]
		expert3_0_act [label="Expert 8 GELU
GPU: 8" shape=rectangle]
		expert3_0_2 [label="Expert 8 Linear2
Output: [B, L/P, d_model]
GPU: 8" shape=rectangle]
	}
	subgraph cluster_expert3_1 {
		label="Expert 9"
		expert3_1_1 [label="Expert 9 Linear1
Output: [B, L/P, ffn_dim]
GPU: 9" shape=rectangle]
		expert3_1_act [label="Expert 9 GELU
GPU: 9" shape=rectangle]
		expert3_1_2 [label="Expert 9 Linear2
Output: [B, L/P, d_model]
GPU: 9" shape=rectangle]
	}
	subgraph cluster_expert3_2 {
		label="Expert 10"
		expert3_2_1 [label="Expert 10 Linear1
Output: [B, L/P, ffn_dim]
GPU: 10" shape=rectangle]
		expert3_2_act [label="Expert 10 GELU
GPU: 10" shape=rectangle]
		expert3_2_2 [label="Expert 10 Linear2
Output: [B, L/P, d_model]
GPU: 10" shape=rectangle]
	}
	subgraph cluster_expert3_3 {
		label="Expert 11"
		expert3_3_1 [label="Expert 11 Linear1
Output: [B, L/P, ffn_dim]
GPU: 11" shape=rectangle]
		expert3_3_act [label="Expert 11 GELU
GPU: 11" shape=rectangle]
		expert3_3_2 [label="Expert 11 Linear2
Output: [B, L/P, d_model]
GPU: 11" shape=rectangle]
	}
	subgraph cluster_expert3_4 {
		label="Expert 12"
		expert3_4_1 [label="Expert 12 Linear1
Output: [B, L/P, ffn_dim]
GPU: 12" shape=rectangle]
		expert3_4_act [label="Expert 12 GELU
GPU: 12" shape=rectangle]
		expert3_4_2 [label="Expert 12 Linear2
Output: [B, L/P, d_model]
GPU: 12" shape=rectangle]
	}
	subgraph cluster_expert3_5 {
		label="Expert 13"
		expert3_5_1 [label="Expert 13 Linear1
Output: [B, L/P, ffn_dim]
GPU: 13" shape=rectangle]
		expert3_5_act [label="Expert 13 GELU
GPU: 13" shape=rectangle]
		expert3_5_2 [label="Expert 13 Linear2
Output: [B, L/P, d_model]
GPU: 13" shape=rectangle]
	}
	subgraph cluster_expert3_6 {
		label="Expert 14"
		expert3_6_1 [label="Expert 14 Linear1
Output: [B, L/P, ffn_dim]
GPU: 14" shape=rectangle]
		expert3_6_act [label="Expert 14 GELU
GPU: 14" shape=rectangle]
		expert3_6_2 [label="Expert 14 Linear2
Output: [B, L/P, d_model]
GPU: 14" shape=rectangle]
	}
	subgraph cluster_expert3_7 {
		label="Expert 15"
		expert3_7_1 [label="Expert 15 Linear1
Output: [B, L/P, ffn_dim]
GPU: 15" shape=rectangle]
		expert3_7_act [label="Expert 15 GELU
GPU: 15" shape=rectangle]
		expert3_7_2 [label="Expert 15 Linear2
Output: [B, L/P, d_model]
GPU: 15" shape=rectangle]
	}
	subgraph cluster_moe3 {
		label="Mixture of Experts"
		gate3 [label="Gate
Compute routing scores
Input: [B, L/P, d_model]
GPU: 8-15" shape=diamond]
		select3 [label="Select Top-2 Experts
GPU: 8-15" shape=parallelogram]
		combine3 [label="Combine Expert Outputs
Weighted sum by gate scores
GPU: 8-15" shape=parallelogram]
	}
	subgraph cluster_layer3 {
		label="Layer 3"
		res3 [label="Residual Add
Input: [B, L/P, d_model]
GPU: 8-15" shape=parallelogram]
		res_moe3 [label="Residual Add
Input: [B, L/P, d_model]
GPU: 8-15" shape=parallelogram]
	}
	subgraph cluster_stage1 {
		label="Pipeline Stage 1 (GPUs 8-15)"
	}
	subgraph cluster_output {
		label="Output Layer"
		gather [label="Gather from Pipeline
X: [B, L, d_model]
GPU: 8-15 â†’ Host" shape=parallelogram]
		output [label="Output
X: [B, L, d_model]
GPU: Host" shape=parallelogram]
	}
	input -> split0
	split0 -> qkv0_0
	qkv0_0 -> attn0_0
	attn0_0 -> out0_0
	out0_0 -> ar0
	split0 -> qkv0_1
	qkv0_1 -> attn0_1
	attn0_1 -> out0_1
	out0_1 -> ar0
	split0 -> qkv0_2
	qkv0_2 -> attn0_2
	attn0_2 -> out0_2
	out0_2 -> ar0
	split0 -> qkv0_3
	qkv0_3 -> attn0_3
	attn0_3 -> out0_3
	out0_3 -> ar0
	split0 -> qkv0_4
	qkv0_4 -> attn0_4
	attn0_4 -> out0_4
	out0_4 -> ar0
	split0 -> qkv0_5
	qkv0_5 -> attn0_5
	attn0_5 -> out0_5
	out0_5 -> ar0
	split0 -> qkv0_6
	qkv0_6 -> attn0_6
	attn0_6 -> out0_6
	out0_6 -> ar0
	split0 -> qkv0_7
	qkv0_7 -> attn0_7
	attn0_7 -> out0_7
	out0_7 -> ar0
	ar0 -> res0
	split0 -> res0
	res0 -> gate0
	gate0 -> select0
	select0 -> expert0_0_1 [label="if selected" style=dashed]
	expert0_0_1 -> expert0_0_act
	expert0_0_act -> expert0_0_2
	expert0_0_2 -> combine0
	select0 -> expert0_1_1 [label="if selected" style=dashed]
	expert0_1_1 -> expert0_1_act
	expert0_1_act -> expert0_1_2
	expert0_1_2 -> combine0
	select0 -> expert0_2_1 [label="if selected" style=dashed]
	expert0_2_1 -> expert0_2_act
	expert0_2_act -> expert0_2_2
	expert0_2_2 -> combine0
	select0 -> expert0_3_1 [label="if selected" style=dashed]
	expert0_3_1 -> expert0_3_act
	expert0_3_act -> expert0_3_2
	expert0_3_2 -> combine0
	select0 -> expert0_4_1 [label="if selected" style=dashed]
	expert0_4_1 -> expert0_4_act
	expert0_4_act -> expert0_4_2
	expert0_4_2 -> combine0
	select0 -> expert0_5_1 [label="if selected" style=dashed]
	expert0_5_1 -> expert0_5_act
	expert0_5_act -> expert0_5_2
	expert0_5_2 -> combine0
	select0 -> expert0_6_1 [label="if selected" style=dashed]
	expert0_6_1 -> expert0_6_act
	expert0_6_act -> expert0_6_2
	expert0_6_2 -> combine0
	select0 -> expert0_7_1 [label="if selected" style=dashed]
	expert0_7_1 -> expert0_7_act
	expert0_7_act -> expert0_7_2
	expert0_7_2 -> combine0
	combine0 -> res_moe0
	res0 -> res_moe0
	res_moe0 -> send_stage0
	send_stage0 -> recv_stage1
	recv_stage1 -> qkv2_8
	qkv2_8 -> attn2_8
	attn2_8 -> out2_8
	out2_8 -> ar2
	recv_stage1 -> qkv2_9
	qkv2_9 -> attn2_9
	attn2_9 -> out2_9
	out2_9 -> ar2
	recv_stage1 -> qkv2_10
	qkv2_10 -> attn2_10
	attn2_10 -> out2_10
	out2_10 -> ar2
	recv_stage1 -> qkv2_11
	qkv2_11 -> attn2_11
	attn2_11 -> out2_11
	out2_11 -> ar2
	recv_stage1 -> qkv2_12
	qkv2_12 -> attn2_12
	attn2_12 -> out2_12
	out2_12 -> ar2
	recv_stage1 -> qkv2_13
	qkv2_13 -> attn2_13
	attn2_13 -> out2_13
	out2_13 -> ar2
	recv_stage1 -> qkv2_14
	qkv2_14 -> attn2_14
	attn2_14 -> out2_14
	out2_14 -> ar2
	recv_stage1 -> qkv2_15
	qkv2_15 -> attn2_15
	attn2_15 -> out2_15
	out2_15 -> ar2
	ar2 -> res2
	recv_stage1 -> res2
	res2 -> gate2
	gate2 -> select2
	select2 -> expert2_0_1 [label="if selected" style=dashed]
	expert2_0_1 -> expert2_0_act
	expert2_0_act -> expert2_0_2
	expert2_0_2 -> combine2
	select2 -> expert2_1_1 [label="if selected" style=dashed]
	expert2_1_1 -> expert2_1_act
	expert2_1_act -> expert2_1_2
	expert2_1_2 -> combine2
	select2 -> expert2_2_1 [label="if selected" style=dashed]
	expert2_2_1 -> expert2_2_act
	expert2_2_act -> expert2_2_2
	expert2_2_2 -> combine2
	select2 -> expert2_3_1 [label="if selected" style=dashed]
	expert2_3_1 -> expert2_3_act
	expert2_3_act -> expert2_3_2
	expert2_3_2 -> combine2
	select2 -> expert2_4_1 [label="if selected" style=dashed]
	expert2_4_1 -> expert2_4_act
	expert2_4_act -> expert2_4_2
	expert2_4_2 -> combine2
	select2 -> expert2_5_1 [label="if selected" style=dashed]
	expert2_5_1 -> expert2_5_act
	expert2_5_act -> expert2_5_2
	expert2_5_2 -> combine2
	select2 -> expert2_6_1 [label="if selected" style=dashed]
	expert2_6_1 -> expert2_6_act
	expert2_6_act -> expert2_6_2
	expert2_6_2 -> combine2
	select2 -> expert2_7_1 [label="if selected" style=dashed]
	expert2_7_1 -> expert2_7_act
	expert2_7_act -> expert2_7_2
	expert2_7_2 -> combine2
	combine2 -> res_moe2
	res2 -> res_moe2
	res_moe2 -> qkv3_8
	qkv3_8 -> attn3_8
	attn3_8 -> out3_8
	out3_8 -> ar3
	res_moe2 -> qkv3_9
	qkv3_9 -> attn3_9
	attn3_9 -> out3_9
	out3_9 -> ar3
	res_moe2 -> qkv3_10
	qkv3_10 -> attn3_10
	attn3_10 -> out3_10
	out3_10 -> ar3
	res_moe2 -> qkv3_11
	qkv3_11 -> attn3_11
	attn3_11 -> out3_11
	out3_11 -> ar3
	res_moe2 -> qkv3_12
	qkv3_12 -> attn3_12
	attn3_12 -> out3_12
	out3_12 -> ar3
	res_moe2 -> qkv3_13
	qkv3_13 -> attn3_13
	attn3_13 -> out3_13
	out3_13 -> ar3
	res_moe2 -> qkv3_14
	qkv3_14 -> attn3_14
	attn3_14 -> out3_14
	out3_14 -> ar3
	res_moe2 -> qkv3_15
	qkv3_15 -> attn3_15
	attn3_15 -> out3_15
	out3_15 -> ar3
	ar3 -> res3
	res_moe2 -> res3
	res3 -> gate3
	gate3 -> select3
	select3 -> expert3_0_1 [label="if selected" style=dashed]
	expert3_0_1 -> expert3_0_act
	expert3_0_act -> expert3_0_2
	expert3_0_2 -> combine3
	select3 -> expert3_1_1 [label="if selected" style=dashed]
	expert3_1_1 -> expert3_1_act
	expert3_1_act -> expert3_1_2
	expert3_1_2 -> combine3
	select3 -> expert3_2_1 [label="if selected" style=dashed]
	expert3_2_1 -> expert3_2_act
	expert3_2_act -> expert3_2_2
	expert3_2_2 -> combine3
	select3 -> expert3_3_1 [label="if selected" style=dashed]
	expert3_3_1 -> expert3_3_act
	expert3_3_act -> expert3_3_2
	expert3_3_2 -> combine3
	select3 -> expert3_4_1 [label="if selected" style=dashed]
	expert3_4_1 -> expert3_4_act
	expert3_4_act -> expert3_4_2
	expert3_4_2 -> combine3
	select3 -> expert3_5_1 [label="if selected" style=dashed]
	expert3_5_1 -> expert3_5_act
	expert3_5_act -> expert3_5_2
	expert3_5_2 -> combine3
	select3 -> expert3_6_1 [label="if selected" style=dashed]
	expert3_6_1 -> expert3_6_act
	expert3_6_act -> expert3_6_2
	expert3_6_2 -> combine3
	select3 -> expert3_7_1 [label="if selected" style=dashed]
	expert3_7_1 -> expert3_7_act
	expert3_7_act -> expert3_7_2
	expert3_7_2 -> combine3
	combine3 -> res_moe3
	res3 -> res_moe3
	res_moe3 -> gather
	gather -> output
}
