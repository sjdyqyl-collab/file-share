digraph proposed_moe_final {
	nodesep=0.4 rankdir=TB ranksep=1.0 size="50,60"
	input [label="Input
[1024 tokens, hidden_dim]
Broadcast to 64 GPUs" fillcolor=lightcoral shape=ellipse]
	subgraph layer1 {
		label="Layer 1 (Experts 0-15 on GPUs 0-15)"
		l1_attn [label="Multi-Head Attention
[1024 tokens, hidden_dim]
Replicated on GPUs 0-15" fillcolor=lightblue]
		l1_gate [label="Gate Network
[1024 tokens]
Compute routing probabilities
GPUs 0-15" shape=parallelogram]
		l1_split [label="Async Token Split
[Variable tokens per expert]
Send to target GPUs" shape=parallelogram]
		l1_expert0_gpu0 [label="Expert 0
GPU 0
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert1_gpu1 [label="Expert 1
GPU 1
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert2_gpu2 [label="Expert 2
GPU 2
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert3_gpu3 [label="Expert 3
GPU 3
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert4_gpu4 [label="Expert 4
GPU 4
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert5_gpu5 [label="Expert 5
GPU 5
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert6_gpu6 [label="Expert 6
GPU 6
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert7_gpu7 [label="Expert 7
GPU 7
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert8_gpu8 [label="Expert 8
GPU 8
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert9_gpu9 [label="Expert 9
GPU 9
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert10_gpu10 [label="Expert 10
GPU 10
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert11_gpu11 [label="Expert 11
GPU 11
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert12_gpu12 [label="Expert 12
GPU 12
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert13_gpu13 [label="Expert 13
GPU 13
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert14_gpu14 [label="Expert 14
GPU 14
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_expert15_gpu15 [label="Expert 15
GPU 15
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l1_gather [label="Async All-Gather
[1024 tokens]
Collect from GPUs 0-15" shape=parallelogram]
		l1_residual [label="Residual Add
[1024 tokens]
Across GPUs 0-15" shape=diamond]
	}
	subgraph layer2 {
		label="Layer 2 (Experts 16-31 on GPUs 16-31)"
		l2_attn [label="Multi-Head Attention
[1024 tokens, hidden_dim]
Replicated on GPUs 16-31" fillcolor=lightblue]
		l2_gate [label="Gate Network
[1024 tokens]
Compute routing probabilities
GPUs 16-31" shape=parallelogram]
		l2_split [label="Async Token Split
[Variable tokens per expert]
Send to target GPUs" shape=parallelogram]
		l2_expert0_gpu16 [label="Expert 16
GPU 16
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert1_gpu17 [label="Expert 17
GPU 17
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert2_gpu18 [label="Expert 18
GPU 18
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert3_gpu19 [label="Expert 19
GPU 19
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert4_gpu20 [label="Expert 20
GPU 20
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert5_gpu21 [label="Expert 21
GPU 21
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert6_gpu22 [label="Expert 22
GPU 22
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert7_gpu23 [label="Expert 23
GPU 23
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert8_gpu24 [label="Expert 24
GPU 24
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert9_gpu25 [label="Expert 25
GPU 25
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert10_gpu26 [label="Expert 26
GPU 26
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert11_gpu27 [label="Expert 27
GPU 27
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert12_gpu28 [label="Expert 28
GPU 28
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert13_gpu29 [label="Expert 29
GPU 29
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert14_gpu30 [label="Expert 30
GPU 30
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_expert15_gpu31 [label="Expert 31
GPU 31
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l2_gather [label="Async All-Gather
[1024 tokens]
Collect from GPUs 16-31" shape=parallelogram]
		l2_residual [label="Residual Add
[1024 tokens]
Across GPUs 16-31" shape=diamond]
	}
	subgraph layer3 {
		label="Layer 3 (Experts 32-47 on GPUs 32-47)"
		l3_attn [label="Multi-Head Attention
[1024 tokens, hidden_dim]
Replicated on GPUs 32-47" fillcolor=lightblue]
		l3_gate [label="Gate Network
[1024 tokens]
Compute routing probabilities
GPUs 32-47" shape=parallelogram]
		l3_split [label="Async Token Split
[Variable tokens per expert]
Send to target GPUs" shape=parallelogram]
		l3_expert0_gpu32 [label="Expert 32
GPU 32
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert1_gpu33 [label="Expert 33
GPU 33
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert2_gpu34 [label="Expert 34
GPU 34
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert3_gpu35 [label="Expert 35
GPU 35
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert4_gpu36 [label="Expert 36
GPU 36
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert5_gpu37 [label="Expert 37
GPU 37
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert6_gpu38 [label="Expert 38
GPU 38
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert7_gpu39 [label="Expert 39
GPU 39
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert8_gpu40 [label="Expert 40
GPU 40
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert9_gpu41 [label="Expert 41
GPU 41
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert10_gpu42 [label="Expert 42
GPU 42
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert11_gpu43 [label="Expert 43
GPU 43
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert12_gpu44 [label="Expert 44
GPU 44
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert13_gpu45 [label="Expert 45
GPU 45
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert14_gpu46 [label="Expert 46
GPU 46
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_expert15_gpu47 [label="Expert 47
GPU 47
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l3_gather [label="Async All-Gather
[1024 tokens]
Collect from GPUs 32-47" shape=parallelogram]
		l3_residual [label="Residual Add
[1024 tokens]
Across GPUs 32-47" shape=diamond]
	}
	subgraph layer4 {
		label="Layer 4 (Experts 48-63 on GPUs 48-63)"
		l4_attn [label="Multi-Head Attention
[1024 tokens, hidden_dim]
Replicated on GPUs 48-63" fillcolor=lightblue]
		l4_gate [label="Gate Network
[1024 tokens]
Compute routing probabilities
GPUs 48-63" shape=parallelogram]
		l4_split [label="Async Token Split
[Variable tokens per expert]
Send to target GPUs" shape=parallelogram]
		l4_expert0_gpu48 [label="Expert 48
GPU 48
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert1_gpu49 [label="Expert 49
GPU 49
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert2_gpu50 [label="Expert 50
GPU 50
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert3_gpu51 [label="Expert 51
GPU 51
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert4_gpu52 [label="Expert 52
GPU 52
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert5_gpu53 [label="Expert 53
GPU 53
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert6_gpu54 [label="Expert 54
GPU 54
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert7_gpu55 [label="Expert 55
GPU 55
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert8_gpu56 [label="Expert 56
GPU 56
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert9_gpu57 [label="Expert 57
GPU 57
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert10_gpu58 [label="Expert 58
GPU 58
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert11_gpu59 [label="Expert 59
GPU 59
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert12_gpu60 [label="Expert 60
GPU 60
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert13_gpu61 [label="Expert 61
GPU 61
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert14_gpu62 [label="Expert 62
GPU 62
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_expert15_gpu63 [label="Expert 63
GPU 63
[Variable tokens, expert_dim]
1 expert/GPU" fillcolor=lightblue]
		l4_gather [label="Async All-Gather
[1024 tokens]
Collect from GPUs 48-63" shape=parallelogram]
		l4_residual [label="Residual Add
[1024 tokens]
Across GPUs 48-63" shape=diamond]
	}
	output [label="Output
[1024 tokens, hidden_dim]
From GPU 63" fillcolor=lightcoral shape=ellipse]
	input -> l1_attn
	l1_attn -> l1_gate
	l1_gate -> l1_split
	l1_split -> l1_expert0_gpu0 [label="tokens routed to GPU 0" style=dashed]
	l1_split -> l1_expert1_gpu1 [label="tokens routed to GPU 1" style=dashed]
	l1_split -> l1_expert2_gpu2 [label="tokens routed to GPU 2" style=dashed]
	l1_split -> l1_expert3_gpu3 [label="tokens routed to GPU 3" style=dashed]
	l1_split -> l1_expert4_gpu4 [label="tokens routed to GPU 4" style=dashed]
	l1_split -> l1_expert5_gpu5 [label="tokens routed to GPU 5" style=dashed]
	l1_split -> l1_expert6_gpu6 [label="tokens routed to GPU 6" style=dashed]
	l1_split -> l1_expert7_gpu7 [label="tokens routed to GPU 7" style=dashed]
	l1_split -> l1_expert8_gpu8 [label="tokens routed to GPU 8" style=dashed]
	l1_split -> l1_expert9_gpu9 [label="tokens routed to GPU 9" style=dashed]
	l1_split -> l1_expert10_gpu10 [label="tokens routed to GPU 10" style=dashed]
	l1_split -> l1_expert11_gpu11 [label="tokens routed to GPU 11" style=dashed]
	l1_split -> l1_expert12_gpu12 [label="tokens routed to GPU 12" style=dashed]
	l1_split -> l1_expert13_gpu13 [label="tokens routed to GPU 13" style=dashed]
	l1_split -> l1_expert14_gpu14 [label="tokens routed to GPU 14" style=dashed]
	l1_split -> l1_expert15_gpu15 [label="tokens routed to GPU 15" style=dashed]
	l1_expert0_gpu0 -> l1_gather
	l1_expert1_gpu1 -> l1_gather
	l1_expert2_gpu2 -> l1_gather
	l1_expert3_gpu3 -> l1_gather
	l1_expert4_gpu4 -> l1_gather
	l1_expert5_gpu5 -> l1_gather
	l1_expert6_gpu6 -> l1_gather
	l1_expert7_gpu7 -> l1_gather
	l1_expert8_gpu8 -> l1_gather
	l1_expert9_gpu9 -> l1_gather
	l1_expert10_gpu10 -> l1_gather
	l1_expert11_gpu11 -> l1_gather
	l1_expert12_gpu12 -> l1_gather
	l1_expert13_gpu13 -> l1_gather
	l1_expert14_gpu14 -> l1_gather
	l1_expert15_gpu15 -> l1_gather
	l1_gather -> l1_residual
	l1_residual -> l2_attn
	l2_split -> l2_expert0_gpu16 [label="tokens routed to GPU 16" style=dashed]
	l2_split -> l2_expert1_gpu17 [label="tokens routed to GPU 17" style=dashed]
	l2_split -> l2_expert2_gpu18 [label="tokens routed to GPU 18" style=dashed]
	l2_split -> l2_expert3_gpu19 [label="tokens routed to GPU 19" style=dashed]
	l2_split -> l2_expert4_gpu20 [label="tokens routed to GPU 20" style=dashed]
	l2_split -> l2_expert5_gpu21 [label="tokens routed to GPU 21" style=dashed]
	l2_split -> l2_expert6_gpu22 [label="tokens routed to GPU 22" style=dashed]
	l2_split -> l2_expert7_gpu23 [label="tokens routed to GPU 23" style=dashed]
	l2_split -> l2_expert8_gpu24 [label="tokens routed to GPU 24" style=dashed]
	l2_split -> l2_expert9_gpu25 [label="tokens routed to GPU 25" style=dashed]
	l2_split -> l2_expert10_gpu26 [label="tokens routed to GPU 26" style=dashed]
	l2_split -> l2_expert11_gpu27 [label="tokens routed to GPU 27" style=dashed]
	l2_split -> l2_expert12_gpu28 [label="tokens routed to GPU 28" style=dashed]
	l2_split -> l2_expert13_gpu29 [label="tokens routed to GPU 29" style=dashed]
	l2_split -> l2_expert14_gpu30 [label="tokens routed to GPU 30" style=dashed]
	l2_split -> l2_expert15_gpu31 [label="tokens routed to GPU 31" style=dashed]
	l2_expert0_gpu16 -> l2_gather
	l2_expert1_gpu17 -> l2_gather
	l2_expert2_gpu18 -> l2_gather
	l2_expert3_gpu19 -> l2_gather
	l2_expert4_gpu20 -> l2_gather
	l2_expert5_gpu21 -> l2_gather
	l2_expert6_gpu22 -> l2_gather
	l2_expert7_gpu23 -> l2_gather
	l2_expert8_gpu24 -> l2_gather
	l2_expert9_gpu25 -> l2_gather
	l2_expert10_gpu26 -> l2_gather
	l2_expert11_gpu27 -> l2_gather
	l2_expert12_gpu28 -> l2_gather
	l2_expert13_gpu29 -> l2_gather
	l2_expert14_gpu30 -> l2_gather
	l2_expert15_gpu31 -> l2_gather
	l2_gather -> l2_residual
	l2_residual -> l3_attn
	l3_split -> l3_expert0_gpu32 [label="tokens routed to GPU 32" style=dashed]
	l3_split -> l3_expert1_gpu33 [label="tokens routed to GPU 33" style=dashed]
	l3_split -> l3_expert2_gpu34 [label="tokens routed to GPU 34" style=dashed]
	l3_split -> l3_expert3_gpu35 [label="tokens routed to GPU 35" style=dashed]
	l3_split -> l3_expert4_gpu36 [label="tokens routed to GPU 36" style=dashed]
	l3_split -> l3_expert5_gpu37 [label="tokens routed to GPU 37" style=dashed]
	l3_split -> l3_expert6_gpu38 [label="tokens routed to GPU 38" style=dashed]
	l3_split -> l3_expert7_gpu39 [label="tokens routed to GPU 39" style=dashed]
	l3_split -> l3_expert8_gpu40 [label="tokens routed to GPU 40" style=dashed]
	l3_split -> l3_expert9_gpu41 [label="tokens routed to GPU 41" style=dashed]
	l3_split -> l3_expert10_gpu42 [label="tokens routed to GPU 42" style=dashed]
	l3_split -> l3_expert11_gpu43 [label="tokens routed to GPU 43" style=dashed]
	l3_split -> l3_expert12_gpu44 [label="tokens routed to GPU 44" style=dashed]
	l3_split -> l3_expert13_gpu45 [label="tokens routed to GPU 45" style=dashed]
	l3_split -> l3_expert14_gpu46 [label="tokens routed to GPU 46" style=dashed]
	l3_split -> l3_expert15_gpu47 [label="tokens routed to GPU 47" style=dashed]
	l3_expert0_gpu32 -> l3_gather
	l3_expert1_gpu33 -> l3_gather
	l3_expert2_gpu34 -> l3_gather
	l3_expert3_gpu35 -> l3_gather
	l3_expert4_gpu36 -> l3_gather
	l3_expert5_gpu37 -> l3_gather
	l3_expert6_gpu38 -> l3_gather
	l3_expert7_gpu39 -> l3_gather
	l3_expert8_gpu40 -> l3_gather
	l3_expert9_gpu41 -> l3_gather
	l3_expert10_gpu42 -> l3_gather
	l3_expert11_gpu43 -> l3_gather
	l3_expert12_gpu44 -> l3_gather
	l3_expert13_gpu45 -> l3_gather
	l3_expert14_gpu46 -> l3_gather
	l3_expert15_gpu47 -> l3_gather
	l3_gather -> l3_residual
	l3_residual -> l4_attn
	l4_split -> l4_expert0_gpu48 [label="tokens routed to GPU 48" style=dashed]
	l4_split -> l4_expert1_gpu49 [label="tokens routed to GPU 49" style=dashed]
	l4_split -> l4_expert2_gpu50 [label="tokens routed to GPU 50" style=dashed]
	l4_split -> l4_expert3_gpu51 [label="tokens routed to GPU 51" style=dashed]
	l4_split -> l4_expert4_gpu52 [label="tokens routed to GPU 52" style=dashed]
	l4_split -> l4_expert5_gpu53 [label="tokens routed to GPU 53" style=dashed]
	l4_split -> l4_expert6_gpu54 [label="tokens routed to GPU 54" style=dashed]
	l4_split -> l4_expert7_gpu55 [label="tokens routed to GPU 55" style=dashed]
	l4_split -> l4_expert8_gpu56 [label="tokens routed to GPU 56" style=dashed]
	l4_split -> l4_expert9_gpu57 [label="tokens routed to GPU 57" style=dashed]
	l4_split -> l4_expert10_gpu58 [label="tokens routed to GPU 58" style=dashed]
	l4_split -> l4_expert11_gpu59 [label="tokens routed to GPU 59" style=dashed]
	l4_split -> l4_expert12_gpu60 [label="tokens routed to GPU 60" style=dashed]
	l4_split -> l4_expert13_gpu61 [label="tokens routed to GPU 61" style=dashed]
	l4_split -> l4_expert14_gpu62 [label="tokens routed to GPU 62" style=dashed]
	l4_split -> l4_expert15_gpu63 [label="tokens routed to GPU 63" style=dashed]
	l4_expert0_gpu48 -> l4_gather
	l4_expert1_gpu49 -> l4_gather
	l4_expert2_gpu50 -> l4_gather
	l4_expert3_gpu51 -> l4_gather
	l4_expert4_gpu52 -> l4_gather
	l4_expert5_gpu53 -> l4_gather
	l4_expert6_gpu54 -> l4_gather
	l4_expert7_gpu55 -> l4_gather
	l4_expert8_gpu56 -> l4_gather
	l4_expert9_gpu57 -> l4_gather
	l4_expert10_gpu58 -> l4_gather
	l4_expert11_gpu59 -> l4_gather
	l4_expert12_gpu60 -> l4_gather
	l4_expert13_gpu61 -> l4_gather
	l4_expert14_gpu62 -> l4_gather
	l4_expert15_gpu63 -> l4_gather
	l4_gather -> l4_residual
	l4_residual -> output
}
