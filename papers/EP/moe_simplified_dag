// Simplified MoE Deployment View
digraph MoE_Simplified_DAG {
	graph [rankdir=LR splines=ortho]
	input_tokens [label="Input Tokens\n1024×512×4096" fillcolor=lightblue shape=box style=filled]
	layer_0 [label="Layer 0\n64 Experts across 64 GPUs\n1 Expert per GPU" fillcolor=lightyellow shape=component style=filled]
	comm_0 [label="Cross-GPU Communication\nToken Routing & Aggregation" fillcolor=lightcoral shape=parallelogram style=filled]
	input_tokens -> layer_0
	layer_0 -> comm_0
	layer_1 [label="Layer 1\n64 Experts across 64 GPUs\n1 Expert per GPU" fillcolor=lightyellow shape=component style=filled]
	comm_1 [label="Cross-GPU Communication\nToken Routing & Aggregation" fillcolor=lightcoral shape=parallelogram style=filled]
	comm_0 -> layer_1
	layer_1 -> comm_1
	layer_2 [label="Layer 2\n64 Experts across 64 GPUs\n1 Expert per GPU" fillcolor=lightyellow shape=component style=filled]
	comm_2 [label="Cross-GPU Communication\nToken Routing & Aggregation" fillcolor=lightcoral shape=parallelogram style=filled]
	comm_1 -> layer_2
	layer_2 -> comm_2
	layer_3 [label="Layer 3\n64 Experts across 64 GPUs\n1 Expert per GPU" fillcolor=lightyellow shape=component style=filled]
	comm_3 [label="Cross-GPU Communication\nToken Routing & Aggregation" fillcolor=lightcoral shape=parallelogram style=filled]
	comm_2 -> layer_3
	layer_3 -> comm_3
	output_tokens [label="Output Tokens\n1024×512×4096" fillcolor=lightpink shape=box style=filled]
	comm_3 -> output_tokens
}
